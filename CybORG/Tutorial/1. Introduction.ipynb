{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing CybORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the CybORG environment, it is necessary to import the CybORG class. CybORG stands for **Cyb**er **O**perations **R**esearch **G**ym, so remember to capitalise correctly when importing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating CybORG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CybORG has to be manually instantiated by calling the class constructor. This must be passed a ScenarioGenerator class, which contains the details of the scenario.\n",
    "For Challenge 3 we will be using the DroneSwarmScenarioGenerator which creates the correct scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/ai/lib/python3.9/site-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "from CybORG.Simulator.Scenarios import DroneSwarmScenarioGenerator\n",
    "\n",
    "sg = DroneSwarmScenarioGenerator()\n",
    "cyborg = CybORG(scenario_generator=sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PettingZoo Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 3 is a multi-agent scenario consisting of several teams of agents. Red team will be attacking the network, Blue team will be defending the network, while Green team represents the network users who will be passing messages to each other via the drone network. For this challenge, the roles of Red and Green will be handled by internal rules-based agents, while your task is to use the external API to train Blue Team.\n",
    "\n",
    "CybORG supports single and multi-agent reinforcement learning. The single agent API is based on OpenAI gym, while the multi-agent API is based on PettingZoo. Since Challenge 3 is a multi-agent scenario, we will thus use the PettingZooParallelWrapper. We do this by instantiating the wrapper and pass in CybORG as the env parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blue_agent_0', 'blue_agent_1', 'blue_agent_2', 'blue_agent_3', 'blue_agent_4', 'blue_agent_5', 'blue_agent_6', 'blue_agent_7', 'blue_agent_8', 'blue_agent_9', 'blue_agent_10', 'blue_agent_11', 'blue_agent_12', 'blue_agent_13', 'blue_agent_14', 'blue_agent_15', 'blue_agent_16', 'blue_agent_17'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/CybORG/CybORG/Agents/Wrappers/PettingZooParallelWrapper.py:241: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  new_obs = np.zeros(obs_length, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "from CybORG.Agents.Wrappers import PettingZooParallelWrapper\n",
    "\n",
    "env = PettingZooParallelWrapper(env=cyborg)\n",
    "obs = env.reset()\n",
    "\n",
    "print(obs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the initial observation by resetting the environment. This ouputs a dictionary whose keys are the names of the various blue agents and whose values are the corresponding agent's observation. We can see below the observation of 'blue_agent_0'. This is in vector form designed for input into a neural network. See the README page for an explanation of what the values in this vector mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 18  1 33 20  0  2 90 73  0\n",
      "  3 83 11  0  4 18  5  0  5 73 23  0  6 42 19  0  7 65 68  0  8 46 90  0\n",
      "  9 32 54  0 10 34 18  0 11 89  3  0 12 66 50  0 13 17 90  0 14 68 28  0\n",
      " 15 29 64  0 16 61 99  0 17 43 98  0]\n"
     ]
    }
   ],
   "source": [
    "print(obs['blue_agent_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the action space for a specific agent, the action_space method is called, passing in the agent name as a string. This returns a Discrete object from OpenAI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(56)\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space('blue_agent_0')\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are given to CybORG via the step method. Just like PettingZoo, this is given a dictionary whose keys are the agent names and whose values are the corresponding actions. This function returns the next observation, rewards for the agents, the done signal for each agent and the info dictionary, which is usually empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "actions = {k:action_space.sample() for k in obs}\n",
    "obs, reward, done, info = env.step(actions)\n",
    "\n",
    "print(reward['blue_agent_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be enough to get you started training agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
